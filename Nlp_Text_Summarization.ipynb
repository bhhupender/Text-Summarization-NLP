{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KK_mbTDYOW2f"
      },
      "outputs": [],
      "source": [
        "# Project: Text Summarization using NLP\n",
        "\n",
        "# Objective:\n",
        "# Build a text summarization tool using NLP techniques to create extractive and abstractive summaries.\n",
        "\n",
        "# Step 1: Import Required Libraries\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import heapq\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNaZxZHFOcem",
        "outputId": "ea3288f8-2454-4771-bdd5-c7368ff1bea7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Load the Text Data\n",
        "def load_text():\n",
        "    text = \"\"\"Natural Language Processing (NLP) is a subfield of artificial intelligence that focuses on the interaction between computers and humans through natural language. The ultimate objective of NLP is to read, decipher, understand, and make sense of human languages in a valuable way. Many challenges in NLP involve natural language understanding, enabling computers to derive meaning from human or natural language input, and others involve natural language generation.\"\"\"\n",
        "    return text"
      ],
      "metadata": {
        "id": "byAhokyxOsXQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Preprocess the Text Data\n",
        "def preprocess_text(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    word_tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords and punctuation\n",
        "    filtered_words = [word for word in word_tokens if word.isalnum() and word not in stop_words]\n",
        "    return filtered_words"
      ],
      "metadata": {
        "id": "GfkDajCWO1OI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Create a Frequency Table\n",
        "def create_frequency_table(filtered_words):\n",
        "    freq_table = {}\n",
        "    for word in filtered_words:\n",
        "        if word.lower() not in freq_table:\n",
        "            freq_table[word.lower()] = 1\n",
        "        else:\n",
        "            freq_table[word.lower()] += 1\n",
        "    return freq_table"
      ],
      "metadata": {
        "id": "SmY5K1h7O9wu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Score Sentences Based on Frequency\n",
        "def score_sentences(text, freq_table):\n",
        "    sentences = sent_tokenize(text)\n",
        "    sentence_scores = {}\n",
        "    for sent in sentences:\n",
        "        for word in nltk.word_tokenize(sent.lower()):\n",
        "            if word in freq_table.keys():\n",
        "                if sent not in sentence_scores:\n",
        "                    sentence_scores[sent] = freq_table[word]\n",
        "                else:\n",
        "                    sentence_scores[sent] += freq_table[word]\n",
        "    return sentence_scores"
      ],
      "metadata": {
        "id": "MG7VjUboPTix"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Generate Extractive Summary\n",
        "def generate_extractive_summary(sentence_scores, threshold=25):\n",
        "    summary_sentences = heapq.nlargest(\n",
        "        int(len(sentence_scores) * 0.3) + 1, sentence_scores, key=sentence_scores.get\n",
        "    )\n",
        "    summary = ' '.join(summary_sentences)\n",
        "    return summary"
      ],
      "metadata": {
        "id": "00xSVj1qPwer"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Generate Abstractive Summary using T5\n",
        "def generate_abstractive_summary(text):\n",
        "    # Load pre-trained T5 model and tokenizer\n",
        "    model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "    tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "\n",
        "    # Preprocess input text for T5\n",
        "    input_text = \"summarize: \" + text\n",
        "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "\n",
        "    # Generate summary\n",
        "    outputs = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return summary"
      ],
      "metadata": {
        "id": "C257JyheP11B"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yq2yHQr8RXdS",
        "outputId": "ee94f4ba-0716-4729-a5b9-9daba36251f4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = load_text()\n",
        "\n",
        "# Extractive Summarization\n",
        "filtered_words = preprocess_text(text)\n",
        "freq_table = create_frequency_table(filtered_words)\n",
        "sentence_scores = score_sentences(text, freq_table)\n",
        "\n",
        "print(\"Sentence Scores:\", sentence_scores)\n",
        "extractive_summary = generate_extractive_summary(sentence_scores)\n",
        "\n",
        "# Abstractive Summarization\n",
        "abstractive_summary = generate_abstractive_summary(text)\n",
        "\n",
        "print(\"Original Text:\\n\", text)\n",
        "print(\"\\nExtractive Summary:\\n\", extractive_summary)\n",
        "print(\"\\nAbstractive Summary:\\n\", abstractive_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjhqDOsfQX96",
        "outputId": "4b51ab70-5918-4878-8a4d-49e549da6387"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Scores: {'Natural Language Processing (NLP) is a subfield of artificial intelligence that focuses on the interaction between computers and humans through natural language.': 33, 'The ultimate objective of NLP is to read, decipher, understand, and make sense of human languages in a valuable way.': 16, 'Many challenges in NLP involve natural language understanding, enabling computers to derive meaning from human or natural language input, and others involve natural language generation.': 50}\n",
            "Original Text:\n",
            " Natural Language Processing (NLP) is a subfield of artificial intelligence that focuses on the interaction between computers and humans through natural language. The ultimate objective of NLP is to read, decipher, understand, and make sense of human languages in a valuable way. Many challenges in NLP involve natural language understanding, enabling computers to derive meaning from human or natural language input, and others involve natural language generation.\n",
            "\n",
            "Extractive Summary:\n",
            " Many challenges in NLP involve natural language understanding, enabling computers to derive meaning from human or natural language input, and others involve natural language generation.\n",
            "\n",
            "Abstractive Summary:\n",
            " the ultimate objective of NLP is to read, decipher, understand, and make sense of human languages in a valuable way. many challenges in NLP involve natural language understanding, enabling computers to derive meaning from human or natural language input.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uEX6OhNASFFx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}